import os
import torch
import shutil
from diffusers import StableDiffusionPipeline
from flash_attention_patch import apply_flash_attention_patch  # cáº§n cÃ³ file nÃ y cÃ¹ng thÆ° má»¥c

# ==== CONFIG ====
BASE_MODEL = "runwayml/stable-diffusion-v1-5"   # model gá»‘c
SAVE_DIR = "/content/drive/MyDrive/Interior-stable-difusion/flash_sd15_full_patched"
# =================

os.makedirs(SAVE_DIR, exist_ok=True)

print("ğŸš€ Äang táº£i model gá»‘c...")
pipe = StableDiffusionPipeline.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16
).to("cuda")

# 1ï¸âƒ£ Patch FlashAttention
print("ğŸ”„ Äang patch FlashAttention vÃ o UNet...")
num_replaced = apply_flash_attention_patch(pipe.unet)
print(f"âœ… ÄÃ£ patch FlashAttention cho UNet ({num_replaced} module(s))")

# 2ï¸âƒ£ LÆ°u pipeline HuggingFace chuáº©n (cÃ³ thá»ƒ load bÃ¬nh thÆ°á»ng)
print("ğŸ’¾ Äang lÆ°u pipeline chuáº©n (diffusers format)...")
pipe.save_pretrained(SAVE_DIR, safe_serialization=False)

# 3ï¸âƒ£ LÆ°u riÃªng state_dict cá»§a UNet (Ä‘á»ƒ khÃ´ng máº¥t cáº¥u trÃºc FlashAttention)
unet_state_path = f"{SAVE_DIR}/unet_flashattn_state_dict.pth"
torch.save(pipe.unet.state_dict(), unet_state_path)
print(f"âœ… ÄÃ£ lÆ°u state_dict UNet: {unet_state_path}")

# 4ï¸âƒ£ LÆ°u báº£n sao file patch Ä‘á»ƒ auto-load vá» sau
if os.path.exists("flash_attention_patch.py"):
    shutil.copy("flash_attention_patch.py", f"{SAVE_DIR}/flash_attention_patch.py")
    print("âœ… ÄÃ£ sao chÃ©p flash_attention_patch.py vÃ o thÆ° má»¥c model")
else:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y flash_attention_patch.py trong thÆ° má»¥c hiá»‡n táº¡i!")

# 5ï¸âƒ£ Kiá»ƒm tra láº¡i sá»‘ lÆ°á»£ng module FlashAttention
flash_count = sum("flash" in m.__class__.__name__.lower() for m in pipe.unet.modules())
print(f"âš¡ Tá»•ng sá»‘ module FlashAttention Ä‘ang hoáº¡t Ä‘á»™ng: {flash_count}")

print(f"ğŸ‰ HoÃ n táº¥t! Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u Ä‘áº§y Ä‘á»§ táº¡i:\n{SAVE_DIR}")